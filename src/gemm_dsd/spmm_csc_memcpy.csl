// This program computes A*B on a height-by-width PE rectangle
// The matrix A in grid CSC format is distributed to every PE via memcpy
// The matrix B is distributed to first row PEs via memcpy
// Pw.0, ..., Pw.h send out the result C_final via memcpy, where h = height-1 and w = width-1.
// Note that this is the right-hand side column of PEs
//
// Each PE receives the local matrices representing A and B and computes A*B locally, then performs a row reduction
// The last column of PEs finally contains the corresponding rows of C and sends its result back to the host
//

// Notation: a PE (Px.y) is labeled as (px = x, py = y)

param memcpyParams: comptime_struct;

param LAUNCH: color; // a routable color for RPC

// local colors
param RXACT_B: color; // py = 0: don't care
                      // py > 0: receive submatrix B from the north

param TXACT_B: color; // py = height-1: don't care
                      // py < height-1: send submatrix B to the south

param RXACT_C: color; // px = 0: don't care
                      // px > 0: receive partial sum A*B from px = 0

param TXACT_C: color; // px = width-1: don't care
                      // px < width-1: send partial sum to east

// local tasks
param COMP: color;     // compute local C = A*B
param REDUCE: color;   // reduce local C = A*B
param EXIT: color;     // entrypoint to leave RPC


// A: sparse N x K matrix
// B: dense K x M matrix (slim: M small)
// C: output N x M matrix

// A grid: Nt x Kt
// B grid: Kt x Mt
// C grid: Nt x Mt
// We set Mt = M

// A uses grid CSC format and predetermined (padded) size 
// A_val_len, A_rowidx_len and A_colptr_len

param Nt:i16;         
param Kt:i16; 
param M:i16;  

param width: i16;
param height: i16;

const fabric = @import_module("<layout>");

fn get_x_coord() u16 {
    return fabric.get_x_coord();
}

fn get_y_coord() u16 {
    return fabric.get_y_coord();
}

const sys_mod = @import_module( "<memcpy_multi/memcpy>", @concat_structs(memcpyParams, .{
     .LAUNCH = LAUNCH,
     .data_type=f32
    }));

const tsc = @import_module("<time>");


////////////////////////////////////////////////////////////////////////////////
// Main memory (48KB)
////////////////////////////////////////////////////////////////////////////////

// A is Nt x Kt
// B is Kt x M
// C is Nt x M

const  _SIZE_B = Kt*M;
const  _SIZE_C = Nt*M;

var A  = @zeros([Nt*Kt]f32);


var B  = @zeros([Kt*M]f32);

// workspace for A*B
var C = @zeros([Nt*M]f32);

// Declare variables for storing the timestamp counter at the start and the end
// of the core computation.
var startBuffer = @zeros([tsc.tsc_size_words]f32);
var finishBuffer = @zeros([tsc.tsc_size_words]f32);

var startBuffer_local = @zeros([tsc.tsc_size_words]u16);
var finishBuffer_local = @zeros([tsc.tsc_size_words]u16);

// (_px, _py) is the coordinate of region of interest, set by the function bcast_B
// which starts the computation
var _px : i16 ;
var _py : i16 ;

// WARNING: export pointers, not arrays
var ptr_A : [*]f32 = &A;
var ptr_B : [*]f32 = &B;
var ptr_C : [*]f32 = &C;
var ptr_startBuffer : [*]f32 = &startBuffer;
var ptr_finishBuffer : [*]f32 = &finishBuffer;


////////////////////////////////////////////////////////////////////////////////
// DSDs
// data-structure descriptors (DSDs), loaded into data-structure registers (DSRs) to configure DSR
// The DSDs are typically put in their own data segment that is placed right above lo-mem.?
//
// The content of a DSR is a DSD, which is a data structure stored in memory.
// A DSR is a numbered hardware register and, like a GPR, is memory mapped.
// DSRs hold DSDs. Their numbers are stored in instruction operand fields, where the DSD held by the DSR
// serves to describe the actual data operand, which is a memory or fabric tensor.
////////////////////////////////////////////////////////////////////////////////

// B buffer
const mem_B_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Kt*M} -> B[i] });

// Receiving B
const fab_recv_B_wdsd = @get_dsd(fabin_dsd, .{
    .extent = _SIZE_B,
    .fabric_color = RXACT_B,
    .input_queue = 0
});

// Sending B
const fab_trans_B_wdsd = @get_dsd(fabout_dsd, .{
    .extent = _SIZE_B,
    .fabric_color = TXACT_B,
    .output_queue = 1
});

// C buffer
const mem_C_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Nt*M} -> C[i] });

// Receiving C
const fab_recv_C_wdsd = @get_dsd(fabin_dsd, .{
    .extent = _SIZE_C,
    .fabric_color = RXACT_C,
    .input_queue = 2
});

// Sending C
const fab_trans_C_wdsd = @get_dsd(fabout_dsd, .{
    .extent = _SIZE_C,
    .fabric_color = TXACT_C,
    .output_queue = 3
});



////////////////////////////////////////////////////////////////////////////////
// Tasks
////////////////////////////////////////////////////////////////////////////////


// All PEs compute local A*B after A and B are received
task f_comp() void {

    // if we are in row inbetween, send over B to south
    if(0 < _py and _py < height-1 ){
        // Todo: see if async works here or if we have to assign a new task+color for a separate microthread
        // Also verify if input/output queue identifiers can overlap with this implementation
        // They can overlap for 2x2 because they are not used simultaneously in a PE
        @fmovs(fab_trans_B_wdsd, mem_B_buf_dsd);
    }

    // C = A * B 
    // for (@range(i32, Nt)) |i| {
    //     for (@range(i32, M)) |j| {
    //         for (@range(i32, Kt)) |k| {
    //             C[i * M + j] += A[i * Kt + k] * B[k * M + j];
    //         }
    //     }
    // }

    // Do an fmacs based local GEMM
    var A_dsd  = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Nt} -> A[i*Kt] });
    //A_dsd = @set_dsd_base_addr(A_dsd, Ap);

    for (@range(i16, Kt)) |k| {
        var C_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Nt} -> C[i*M] });

        for (@range(i16, M)) |j| {
        const b = B[k*M + j];
        @fmacs(C_dsd, C_dsd, A_dsd, b);
        C_dsd = @increment_dsd_offset(C_dsd, 1, f32);
        }
        A_dsd = @increment_dsd_offset(A_dsd, 1, f32);
    }
    
    // activate reduce task after C computation
    @activate(REDUCE);
}


// px = 0: forward local A*B = C to the east
// px = 1 .. width - 2: receive C from west, compute new C and send result to east
// px = width-1: receive C from west and compute new C
task f_reduce() void {

    if (_px == 0){
        // send partial sum to the east and finish (every PE must call f_exit)
        @fmovs(fab_trans_C_wdsd, mem_C_buf_dsd, .{.async=true, .activate = f_exit});
    }else if(_px == width-1){
        // P_width.0, ... , P_width.height: Receive C from west, compute in local buffer and activate exit
        @fadds(mem_C_buf_dsd, fab_recv_C_wdsd, mem_C_buf_dsd, .{.async=true, .activate = f_exit});
    }else{
        // Receive result from west, add local + west C and send to result to east
        @fadds(fab_trans_C_wdsd, fab_recv_C_wdsd, mem_C_buf_dsd, .{.async=true, .activate = f_exit});
    }

    @activate(EXIT);
}

// This defines the h_params structure
var params = @zeros([32]i32);
var index :i32 =  -1;
var num_wavelets: i32 = 0;


// f_launch: reads h_params and sets up the execution
// bcast_B: broadcasts local B to south PEs
// f_comp: computes local A*B
// f_reduce: receives local A*B from west, does computation and sends local A*B to east 
// f_exit: unblock cmd color for every PE such that C dsds can be read on host

fn bcast_B() void {

    tsc.enable_tsc();
    tsc.get_timestamp(&startBuffer_local);

    _px = @as(i16, get_x_coord());
    _py = @as(i16, get_y_coord());

    if (_py == 0){
        // Broadcast B to south PEs when we are in the first row
        @fmovs(fab_trans_B_wdsd, mem_B_buf_dsd, .{.async=true, .activate = f_comp});
    }else if(_py == height-1){
        // Only receive B from north if we are in the last row
        @fmovs(mem_B_buf_dsd, fab_recv_B_wdsd, .{.async=true, .activate = f_comp});
    }else{
        // Receive B from north PE, send B to south in f_comp!
        @fmovs(mem_B_buf_dsd, fab_recv_B_wdsd, .{.async=true, .activate= f_comp});
    }
}

task f_exit() void {
    tsc.get_timestamp(&finishBuffer_local);

    // bitcast to f16 first, then extend to 32-bits
    startBuffer[0] = @as(f32, @bitcast(f16, startBuffer_local[0]));
    startBuffer[1] = @as(f32, @bitcast(f16, startBuffer_local[1]));
    startBuffer[2] = @as(f32, @bitcast(f16, startBuffer_local[2]));

    finishBuffer[0] = @as(f32, @bitcast(f16, finishBuffer_local[0]));
    finishBuffer[1] = @as(f32, @bitcast(f16, finishBuffer_local[1]));
    finishBuffer[2] = @as(f32, @bitcast(f16, finishBuffer_local[2]));

    // the user must unblock cmd color for every PE
    sys_mod.unblock_cmd_stream();
}

// host runtime sends h_params via color LAUNCH
//
// format of h_params
//
//  +---------------------+
//  | # of wvlts          | 1st wavelet
//  +---------------------+
//  | param 1             | 2nd wavelet
//  +---------------------+
//  | param 2             | 3rd wavelet
//  +---------------------+
//  | param 3             | 4th wavelet
//  +---------------------+
//  | ...                 |
//  +---------------------+
//  | ID of the function  | last wavelet
//  +---------------------+
//
task f_launch( data : i32 ) void {
    if (index < 0){
        num_wavelets = data;
        index = 0;
    }else{
        if (index < num_wavelets){
            params[index] = data;
            index +=1;
        }else{
            index = -1;
            // data is the ID of the function
            if (0 == data){
                // WARNING: all PEs must call f_exit()
                bcast_B();
            }
        }
    }
}


comptime {

    // use microthreads to read B and C, so block RXACT_B and RXACT_C
    @block(RXACT_B);
    @block(RXACT_C);

    // bind tasks to colors
    @bind_task(f_comp, COMP);
    @bind_task(f_reduce, REDUCE);

    @bind_task(f_launch, LAUNCH);
    @bind_task(f_exit, EXIT);
}

comptime {
    @export_symbol(ptr_A, "A");
    @export_symbol(ptr_B, "B");
    @export_symbol(ptr_C, "C");
    @export_symbol(ptr_startBuffer, "startBuffer");
    @export_symbol(ptr_finishBuffer, "finishBuffer");
}
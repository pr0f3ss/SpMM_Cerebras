// This example computes A*B on a 2-by-2 rectangle which has P0.0, P0.1, P1.0 and P1.1
// The matrix A is distributed to every PE via memcpy
// The matrix B is distributed to first row PEs via memcpy
// P1.0 and P1.1 send out the local result of the row A*B via memcpy
//
// The host sends matrix A, matrix B into the core rectangle, launches a RPC (remote procedure call) to
// broadcast vector x from 1st row to other rows, computes A*B locally, then performs a row
// reduction to finish C = A*B.

// Notation: a PE (Px.y) is labeled as (px = x, py = y)

param memcpyParams: comptime_struct;

param LAUNCH: color; // a routable color for RPC

// local colors
param RXACT_B: color; // py = 0: don't care
                      // py > 0: receive submatrix B from the north
param TXACT_B: color; // py = 0: send submatrix B to the south

param RXACT_C: color; // px = 0: forward local A*B to the east
                      // px = 1: receive partial sum A*B from px = 0
param TXACT_C: color;   // px = 0: send parital sum to all px = 1

param TXACT_C_FINAL: color; // used in last column of PEs to send C_final to host

// local tasks
param COMP: color;     // compute local C = A*B
param REDUCE: color;   // compute A*B
param DONE: color;     // Send out the result of the submatrix C
param EXIT: color;     // entrypoint to leave RPC


// A: sparse N x K matrix
// B: dense K x M matrix (slim: M small)
// C: output N x M matrix

// A grid: Nt x Kt
// B grid: Kt x Mt
// C grid: Nt x Mt
// We set Mt = M

// A uses grid CSC format and predetermined (padded) size 
// A_val_len, A_rowidx_len and A_colptr_len

param Nt:i32;         
param Kt:i32; 
param M:i32;  

param A_val_len:i32;
param A_rowidx_len:i32;
param A_colptr_len:i32;


const fabric = @import_module("<layout>");

fn get_x_coord() u16 {
    return fabric.get_x_coord();
}

fn get_y_coord() u16 {
    return fabric.get_y_coord();
}

const sys_mod = @import_module( "<memcpy_multi/memcpy>", @concat_structs(memcpyParams, .{
     .LAUNCH = LAUNCH,
     .data_type=f32
    }));


////////////////////////////////////////////////////////////////////////////////
// Main memory (48KB)
////////////////////////////////////////////////////////////////////////////////

// A is Nt x Kt
// B is Kt x M
// C is Nt x M

const  _SIZE_B = Kt*M;
const  _SIZE_C = Nt*M;

var A_val  = @zeros([A_val_len]f32);
// Memcpy limitation: Have to be copied in as f32 as sys_mod requires us to have the same datatype
var A_row_idx  = @zeros([A_rowidx_len]f32);
var A_col_ptr  = @zeros([A_colptr_len]f32);

var B  = @zeros([Kt*M]f32);
var C_final  = @zeros([Nt*M]f32);

// workspace for A*B
var C_temp = @zeros([Nt*M]f32);

// (_px, _py) is the coordinate of region of interest, set by the function bcast_B
// which starts the computation
var _px : i16 ;
var _py : i16 ;

// WARNING: export pointers, not arrays
var ptr_A_val : [*]f32 = &A_val;
var ptr_A_row_idx  : [*]f32 = &A_row_idx;
var ptr_A_col_ptr : [*]f32 = &A_col_ptr;
// todo: check whether this works because they are arrays, or if we just use normal [*]
var ptr_B : [*]f32 = &B;
var ptr_C_final : [*]f32 = &C_final;
var ptr_C_temp : [*]f32 = &C_temp;

////////////////////////////////////////////////////////////////////////////////
// DSDs
// data-structure descriptors (DSDs), loaded into data-structure registers (DSRs) to configure DSR
// The DSDs are typically put in their own data segment that is placed right above lo-mem.?
//
// The content of a DSR is a DSD, which is a data structure stored in memory.
// A DSR is a numbered hardware register and, like a GPR, is memory mapped.
// DSRs hold DSDs. Their numbers are stored in instruction operand fields, where the DSD held by the DSR
// serves to describe the actual data operand, which is a memory or fabric tensor.
////////////////////////////////////////////////////////////////////////////////

// Have to use multi-dimensional memory vector -> mem4d_dsd
const mem_B_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Kt*M} -> B[i] });

// Receiving B
const fab_recv_B_wdsd = @get_dsd(fabin_dsd, .{
    .extent = _SIZE_B,
    .fabric_color = RXACT_B,
    .input_queue = @get_input_queue(0)
});

// Sending B
const fab_trans_B_wdsd = @get_dsd(fabout_dsd, .{
    .extent = _SIZE_B,
    .fabric_color = TXACT_B,
    .output_queue = 1
});

// C_temp buffer
const mem_C_temp_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Nt*M} -> C_temp[i] });

// Receiving C_temp (in last column)
const fab_recv_C_temp_wdsd = @get_dsd(fabin_dsd, .{
    .extent = _SIZE_C,
    .fabric_color = RXACT_C,
    .input_queue = @get_input_queue(2)
});

// Sending C_temp (to right hand side last column PE)
const fab_trans_C_temp_wdsd = @get_dsd(fabout_dsd, .{
    .extent = _SIZE_C,
    .fabric_color = TXACT_C,
    .output_queue = 3
});

// C_final buffer
const mem_C_final_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Nt*M} -> C_final[i] });


////////////////////////////////////////////////////////////////////////////////
// Tasks
////////////////////////////////////////////////////////////////////////////////

const spmm_csc_mod = @import_module( "spmm_csc.csl" , .{  .A_val_len = A_val_len, .A_rowidx_len = A_rowidx_len, .A_colptr_len = A_colptr_len, 
.Nt = Nt,  .Kt = Kt, .M = M});


// All PEs compute local A*B after A and B are received
task f_comp() void {

    // C_temp = A * B 
    spmm_csc_mod.spmm_csc_f32(&A_val, &A_row_idx, &A_col_ptr, &B, &C_temp);

    // activate reduce task after C_temp computation
    @activate(REDUCE);
}


// px = 0: forward local A*B = C_temp to the east
// px = 1: recive C_temp from west and trigger f_done task to compute C_final = C_temp (from east) + C_temp (from local PE)
task f_reduce() void {

    if (0 == _px){
        // send partial sum to the east and finish (every PE must call f_exit)
        @fmovs(fab_trans_C_temp_wdsd, mem_C_temp_buf_dsd, .{.async=true, .activate = f_exit});
    }else{
        // P1.1 and P1.0: Receive C_temp from west and activate done task to compute C_final = C_temp (from east) + C_temp (from local PE)
        @fmovs(mem_C_final_buf_dsd, fab_recv_C_temp_wdsd, .{.async=true, .activate = f_done});
    }
}

// This task computes C_final = C_temp (from east) + C_temp (from local PE)
task f_done() void {

    // add mem_C_final_buf_dsd (east C_temp) and mem_C_temp_buf_dsd (local C_temp) into output mem_C_final_buf_dsd (which will be copied back to device)
    @fadds(mem_C_final_buf_dsd, mem_C_final_buf_dsd, mem_C_temp_buf_dsd);

    @activate(EXIT); // every PE must call f_exit
}


// This defines the h_params structure
var params = @zeros([32]i32);
var index :i32 =  -1;
var num_wavelets: i32 = 0;

// the calling sequence
// px: cols, py: rows
// py = 0/1, px = 0: bcast_B --> f_comp --> f_reduce --> f_exit
// py = 0/1, px = 1: bcast_B --> f_comp --> f_reduce --> f_done --> f_exit
// We add the local results in the last column (px = 1), hence why we need an additional 'done' task in the last column

// bcast_B: broadcasts local B to south PEs
// f_comp: computes local A*B
// f_reduce: sends local A*B to east or receives local A*B from west
// f_done: computes the addition of the local A*B results
// f_exit: unblock cmd color for every PE such that C_final dsds can be read on host

fn bcast_B() void {
    _px = @as(i16, get_x_coord());
    _py = @as(i16, get_y_coord());

    if (0 == _py){
        // broadcast B to south PEs when we are in the first row
        @fmovs(fab_trans_B_wdsd, mem_B_buf_dsd, .{.async=true, .activate = f_comp});
    }else{
        // 0 < _py: receive B from north if we aren't in the first row
        @fmovs(mem_B_buf_dsd, fab_recv_B_wdsd, .{.async=true, .activate = f_comp});
    }
}

task f_exit() void {
    // the user must unblock cmd color for every PE
    sys_mod.unblock_cmd_stream();
}


comptime {

    // use microthreads to read B and C, so block RXACT_B and RXACT_C
    @block(RXACT_B);
    @block(RXACT_C);

    // bind tasks to colors
    @bind_task(f_comp, COMP);
    @bind_task(f_reduce, REDUCE);
    @bind_task(f_done, DONE);

    @bind_task( f_exit, EXIT);
}

comptime {
    @export_symbol(ptr_A_val, "A_val");
    @export_symbol(ptr_A_row_idx, "A_row_idx");
    @export_symbol(ptr_A_col_ptr, "A_col_ptr");
    @export_symbol(ptr_B, "B");
    @export_symbol(ptr_C_final, "C_final");
    @export_symbol(ptr_C_temp, "C_temp");

    // For memcpy
    @export_symbol(bcast_B);
    @rpc(LAUNCH);
}
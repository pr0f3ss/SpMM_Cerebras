// This program computes A*B on a height-by-width PE rectangle
// The matrix A in grid CSC format is distributed to every PE via memcpy
// The matrix B is distributed to first row PEs via memcpy
// Pw.0, ..., Pw.h send out the result C_final via memcpy, where h = height-1 and w = width-1.
// Note that this is the right-hand side column of PEs
//
// Each PE receives the local matrices representing A and B and computes A*B locally, then performs a row reduction
// The last column of PEs finally contains the corresponding rows of C and sends its result back to the host
//

// Notation: a PE (Px.y) is labeled bitcast (px = x, py = y)

param memcpyParams: comptime_struct;

param LAUNCH: color; // a routable color for RPC

// local colors
param RXACT_B: color; // py = 0: don't care
                      // py > 0: receive submatrix B from the north

param TXACT_B: color; // py = height-1: don't care
                      // py < height-1: send submatrix B to the south

param RXACT_C: color; // px = 0: don't care
                      // px > 0: receive partial sum A*B from px = 0

param TXACT_C: color; // px = width-1: don't care
                      // px < width-1: send partial sum to east

const timeStampColor: color = @get_color(7);

// local tasks
param COMP: color;     // compute local C = A*B
param REDUCE: color;   // reduce local C = A*B
param EXIT: color;     // entrypoint to leave RPC


// A: sparse N x K matrix
// B: dense K x M matrix (slim: M small)
// C: output N x M matrix

// A grid: Nt x Kt
// B grid: Kt x Mt
// C grid: Nt x Mt
// We set Mt = M

// A uses grid CSC format and predetermined (padded) size 
// A_val_len, A_rowidx_len and A_colptr_len

param Nt:i32;         
param Kt:i32; 
param M:i32;  

param A_val_len:i32;
param A_rowidx_len:i32;
param A_colptr_len:i32;

param width: i16;
param height: i16;

const fabric = @import_module("<layout>");

fn get_x_coord() u16 {
    return fabric.get_x_coord();
}

fn get_y_coord() u16 {
    return fabric.get_y_coord();
}

const sys_mod = @import_module( "<memcpy_multi/memcpy>", @concat_structs(memcpyParams, .{
     .LAUNCH = LAUNCH,
     .data_type=f32
    }));

const tsc = @import_module("<time>");


////////////////////////////////////////////////////////////////////////////////
// Main memory (48KB)
////////////////////////////////////////////////////////////////////////////////

// A is Nt x Kt
// B is Kt x M
// C is Nt x M

// alignment calculation
const pad_align:   i32 = 16;
const elem_size:   i32 = 4;
const align_ratio: i32 = pad_align / elem_size;
const padded_M:   i32 = if (((M+1) / align_ratio) * align_ratio == (M+1)) (M+1)
                         else ((M+1) / align_ratio + 1) * align_ratio;

const  _SIZE_B = Kt*padded_M;
const  _SIZE_C = Nt*padded_M;


var A_val  = @zeros([A_val_len]f32);
// Memcpy limitation: Have to be copied in bitcast f32 bitcast sys_mod requires us to have the same datatype
var A_row_idx  = @zeros([A_rowidx_len]f32);
var A_col_ptr  = @zeros([A_colptr_len]f32);

var B  = @zeros([Kt*padded_M]f32);

// workspace for A*B
var C = @zeros([Nt*padded_M]f32);

// Declare variables for storing the timestamp counter at the start and the end
// of the core computation.
var time_buf_f32 = @zeros([3]f32);

var tscStartBuffer = @zeros([tsc.tsc_size_words]u16);
var tscEndBuffer = @zeros([tsc.tsc_size_words]u16);

// (_px, _py) is the coordinate of region of interest, set by the function bcast_B
// which starts the computation
var _px : i16 ;
var _py : i16 ;

// WARNING: export pointers, not arrays
var ptr_A_val : [*]f32 = &A_val;
var ptr_A_row_idx  : [*]f32 = &A_row_idx;
var ptr_A_col_ptr : [*]f32 = &A_col_ptr;
var ptr_B : [*]f32 = &B;
var ptr_C : [*]f32 = &C;
var ptr_time_memcpy: [*]f32 = &time_buf_f32;


////////////////////////////////////////////////////////////////////////////////
// DSDs
// data-structure descriptors (DSDs), loaded into data-structure registers (DSRs) to configure DSR
// The DSDs are typically put in their own data segment that is placed right above lo-mem.?
//
// The content of a DSR is a DSD, which is a data structure stored in memory.
// A DSR is a numbered hardware register and, like a GPR, is memory mapped.
// DSRs hold DSDs. Their numbers are stored in instruction operand fields, where the DSD held by the DSR
// serves to describe the actual data operand, which is a memory or fabric tensor.
////////////////////////////////////////////////////////////////////////////////

// B buffer
const mem_B_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Kt*padded_M} -> B[i] });

// Receiving B
const fab_recv_B_wdsd = @get_dsd(fabin_dsd, .{
    .extent = _SIZE_B,
    .fabric_color = RXACT_B,
    .input_queue = @get_input_queue(0)
});

// Sending B
const fab_trans_B_wdsd = @get_dsd(fabout_dsd, .{
    .extent = _SIZE_B,
    .fabric_color = TXACT_B,
    .output_queue = 1
});

// C buffer
const mem_C_buf_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Nt*padded_M} -> C[i] });

// Receiving C
const fab_recv_C_wdsd = @get_dsd(fabin_dsd, .{
    .extent = _SIZE_C,
    .fabric_color = RXACT_C,
    .input_queue = @get_input_queue(2)
});

// Sending C
const fab_trans_C_wdsd = @get_dsd(fabout_dsd, .{
    .extent = _SIZE_C,
    .fabric_color = TXACT_C,
    .output_queue = 3
});



////////////////////////////////////////////////////////////////////////////////
// Tasks
////////////////////////////////////////////////////////////////////////////////


// All PEs compute local A*B after A and B are received
task f_comp() void {

    // if we are in row inbetween, send over B to south
    if(0 < _py and _py < height-1 ){
        @fmovs(fab_trans_B_wdsd, mem_B_buf_dsd, .{.async=true});
    }

    tsc.enable_tsc();
    tsc.get_timestamp(&tscStartBuffer);

    // Set up DSRs from DSDs
    var B_dsd  = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{padded_M} -> B[i] });
    var C_dsd  = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{padded_M} -> C[i+1] });

    const B_dsr = @get_dsr(dsr_src1, 0);
    const C_dsr = @get_dsr(dsr_src0, 1);

    @load_to_dsr(B_dsr, B_dsd);
    @load_to_dsr(C_dsr, C_dsd);

    // C = A * B
    // iterate over column pointers
    for (@range(i32, A_colptr_len-1)) |j| {

        // get number of non-zero rows in the current column
        // cast from f32 to i32
        var row_elems = @bitcast(i32, A_col_ptr[j+1] - A_col_ptr[j]);

        var row_idx_start = @bitcast(i32, A_col_ptr[j]);

        // Exit, if we reach padded end
        if(@bitcast(i32, A_col_ptr[j+1]) == -1){
            break;
        }

        // iterate over all non-zero rows in the current column
        for (@range(i32, row_elems)) |i| {

            // get the reference element row index
            var ref_elem_row_idx = @bitcast(i32, A_row_idx[row_idx_start+i]);

            // extract the referenced non-zero value
            var a = A_val[row_idx_start+i];

            // get a's coordinates
            var a_i : i32 = ref_elem_row_idx;   // row
            var a_j : i32 = j;                  // col

            // Set base address to the rows we need
            @set_dsr_base_addr(B_dsr, @ptrcast([*]f32, &(B[padded_M*a_j])));
            @set_dsr_base_addr(C_dsr, @ptrcast([*]f32, &(C[padded_M*a_i])));

            @fmacs(C_dsr, C_dsr, B_dsr, a);
        }
    }

    tsc.get_timestamp(&tscEndBuffer);

    // activate reduce task after C computation
    @activate(REDUCE);
}


// px = 0: forward local A*B = C to the east
// px = 1 .. width - 2: receive C from west, compute new C and send result to east
// px = width-1: receive C from west and compute new C
task f_reduce() void {
    if (_px == 0){
        // send partial sum to the east and finish (every PE must call f_exit)
        @fmovs(fab_trans_C_wdsd, mem_C_buf_dsd, .{.async=true, .activate = f_exit});
    }else if(_px == width-1){
        // P_width.0, ... , P_width.height: Receive C from west, compute in local buffer and activate exit
        @fadds(mem_C_buf_dsd, fab_recv_C_wdsd, mem_C_buf_dsd, .{.async=true, .activate = f_exit});
    }else{
        // Receive result from west, add local + west C and send to result to east
        @fadds(fab_trans_C_wdsd, fab_recv_C_wdsd, mem_C_buf_dsd, .{.async=true, .activate = f_exit});
    }
}

// bcast_B: broadcasts local B to south PEs
// f_comp: computes local A*B
// f_reduce: receives local A*B from west, does computation and sends local A*B to east 
// f_exit: unblock cmd color for every PE such that C dsds can be read on host

fn bcast_B() void {
    _px = @as(i16, get_x_coord());
    _py = @as(i16, get_y_coord());

    if (_py == 0){
        // Broadcast B to south PEs when we are in the first row
        @fmovs(fab_trans_B_wdsd, mem_B_buf_dsd, .{.async=true, .activate = f_comp});
    }else if(_py == height-1){
        // Only receive B from north if we are in the last row
        @fmovs(mem_B_buf_dsd, fab_recv_B_wdsd, .{.async=true, .activate = f_comp});
    }else{
        // Receive B from north PE, send B to south in f_comp!
        @fmovs(mem_B_buf_dsd, fab_recv_B_wdsd, .{.async=true, .activate= f_comp});
    }
}

task f_exit() void {
    var lo_ : u16 = 0;
    var hi_ : u16 = 0;
    var word : u32 = 0;

    lo_ = tscStartBuffer[0];
    hi_ = tscStartBuffer[1];
    time_buf_f32[0] = @as(f32, (@as(u32,hi_) << @as(u16,16)) | @as(u32, lo_) );

    lo_ = tscStartBuffer[2];
    hi_ = tscEndBuffer[0];
    time_buf_f32[1] = @as(f32, (@as(u32,hi_) << @as(u16,16)) | @as(u32, lo_) );

    lo_ = tscEndBuffer[1];
    hi_ = tscEndBuffer[2];
    time_buf_f32[2] = @as(f32, (@as(u32,hi_) << @as(u16,16)) | @as(u32, lo_) );

    // the user must unblock cmd color for every PE
    sys_mod.unblock_cmd_stream();
}

comptime {

    // use microthreads to read B and C, so block RXACT_B and RXACT_C
    @block(RXACT_B);
    @block(RXACT_C);

    // bind tasks to colors
    @bind_task(f_comp, COMP);
    @bind_task(f_reduce, REDUCE);

    @bind_task(f_exit, EXIT);
}

comptime {
    @export_symbol(ptr_A_val, "A_val");
    @export_symbol(ptr_A_row_idx, "A_row_idx");
    @export_symbol(ptr_A_col_ptr, "A_col_ptr");
    @export_symbol(ptr_B, "B");
    @export_symbol(ptr_C, "C");
    @export_symbol(ptr_time_memcpy, "time_memcpy");

    // For memcpy
    @export_symbol(bcast_B);
    @rpc(LAUNCH);
}